{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of Experimenmt Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "- Replace your AWS credentials\n",
    "- Specify the name of your S3 bucket\n",
    "- Print available experiemnt clusters to download locally\n",
    "- Pick your experiment cluster to plot\n",
    "- Pick which experiments in the cluster to plot\n",
    "- Make your plots\n",
    "- Analyze!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logfile(logfile):\n",
    "    # so the tricky part we have to deal with in these log files\n",
    "    # is that the job could crash and get restarted, which will\n",
    "    # re-wind back and start re-logging older steps. So we keep\n",
    "    # all the data as dictionary and over-write old data with new\n",
    "    # and then at the end compile everything together\n",
    "\n",
    "    # read raw data\n",
    "    streams = {} # stream:str -> {step: val}\n",
    "    lrs = {}\n",
    "    total_training_time = 0\n",
    "    with open(logfile, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            step = int(parts[0].split(\":\")[1])\n",
    "            stream = parts[1].split(\":\")[0]\n",
    "            val = float(parts[1].split(\":\")[1])\n",
    "            if not stream in streams:\n",
    "                streams[stream] = {}\n",
    "            d = streams[stream]\n",
    "            d[step] = val\n",
    "            \n",
    "            if len(parts) > 2:\n",
    "                lr = float(parts[2].split(\":\")[1])\n",
    "                lrs[step] = lr\n",
    "                \n",
    "                try:\n",
    "                    time = float(parts[5].split(\":\")[1])\n",
    "                    total_training_time += time / 1000 # convert from ms to s\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "    # now re-represent as list of (step, val) tuples\n",
    "    streams_xy = {}\n",
    "    for k, v in streams.items():\n",
    "        # get all (step, val) items, sort them\n",
    "        xy = sorted(list(v.items()))\n",
    "        # unpack the list of tuples to tuple of lists\n",
    "        streams_xy[k] = zip(*xy)\n",
    "    # return the xs, ys lists\n",
    "\n",
    "    # convert total train time from s to hours\n",
    "    total_training_time /= 3600\n",
    "\n",
    "    return streams_xy, (np.array(list(lrs.keys())), np.array(list(lrs.values()))), total_training_time\n",
    "\n",
    "# optional function that smooths out the loss some\n",
    "def smooth_moving_average(signal, window_size):\n",
    "    if signal.ndim != 1:\n",
    "        raise ValueError(\"smooth_moving_average only accepts 1D arrays.\")\n",
    "    if signal.size < window_size:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "    if window_size < 3:\n",
    "        return signal\n",
    "\n",
    "    s = np.pad(signal, (window_size//2, window_size-1-window_size//2), mode='edge')\n",
    "    w = np.ones(window_size) / window_size\n",
    "    smoothed_signal = np.convolve(s, w, mode='valid')\n",
    "    return smoothed_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_learning_rate(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Check if the line contains the learning rate parameter\n",
    "                if 'learning rate (LR)' in line:\n",
    "                    # Split the line and extract the value\n",
    "                    parts = line.split('|')\n",
    "                    learning_rate = parts[2].strip()  # The value is in the 3rd part\n",
    "                    return float(learning_rate)  # Convert to float for numerical use\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return None  # Return None if the learning rate is not found\n",
    "\n",
    "def extract_parameters(file_path):\n",
    "    num_parameters = None    \n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Check if the line contains the number of parameters\n",
    "                if 'num_parameters' in line:\n",
    "                    # Split the line and extract the value\n",
    "                    parts = line.split('|')\n",
    "                    num_parameters = int(parts[2].strip())  # Convert to int\n",
    "\n",
    "                if num_parameters is not None:\n",
    "                    break\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return num_parameters  # Return both values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your IAM User credentials\n",
    "aws_access_key = \"\"\n",
    "aws_secret_key = \"\"\n",
    "aws_region = \"\"  # Replace with the region that your s3 bucket is in, is normally just us-east-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your logs from the S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boto3 session with your credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "\n",
    "# Create an S3 client or resource with this session\n",
    "s3_client = session.client('s3')\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify your bucket\n",
    "bucket_name = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Available Experiment Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the S3 bucket\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Set to store unique top-level folder names\n",
    "folders = set()\n",
    "\n",
    "# List folders inside the experiments folder\n",
    "experiments_folder = \"experiments/\"\n",
    "for obj in bucket.objects.filter(Prefix=experiments_folder):\n",
    "    # Extract the folder name after 'experiments/' by splitting on '/'\n",
    "    parts = obj.key[len(experiments_folder):].split('/')\n",
    "    if len(parts) > 1:\n",
    "        folder_name = parts[0]\n",
    "        folders.add(folder_name)\n",
    "\n",
    "# Print all unique top-level folders\n",
    "print(\"Experiment Clusters that you can plot:\")\n",
    "for folder in sorted(folders):\n",
    "    print('\\t-', folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting your experiment logs\n",
    "\n",
    "To download your logs, specify the **cluster_name** variable to the experiment cluster you want to download then run the next cell. This downloads this experiment cluster and saves it locally in a folder in the current working directory called downloaded_logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify which experiment cluster you want to download and then plot.\n",
    "cluster_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local directory to save files\n",
    "local_directory = os.path.join(os.getcwd(), \"downloaded_logs\")\n",
    "os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# Download files in the specified subfolder\n",
    "for obj in bucket.objects.filter(Prefix=f\"{experiments_folder}{cluster_name}/\"):\n",
    "    # Define the local path to save the file\n",
    "    relative_path = os.path.relpath(obj.key, experiments_folder)\n",
    "    local_file_path = os.path.join(local_directory, relative_path)\n",
    "\n",
    "    # Create local directories if they do not exist\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "    # Download the file\n",
    "    print(f\"Downloading {obj.key} to {local_file_path}\")\n",
    "    bucket.download_file(obj.key, local_file_path)\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing the individual experiments within your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all entries in the specified directory\n",
    "path = os.path.join('downloaded_logs', cluster_name)\n",
    "\n",
    "print(\"Available individual experiments to plot\")\n",
    "for entry in os.listdir(path):\n",
    "    # Check if the entry is a directory\n",
    "    if os.path.isdir(os.path.join(path, entry)):\n",
    "        print('\\t-', entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which individual experiments from the cluster you would like to plot. Leave the variable **experiments_to_plot** as an empty list if you want to plot all experiments in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Specify which experiments you want to plot. Leave empty is if you want to plot all experiments in the cluster\n",
    "experiments_to_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment(cluster_name, to_plot, plot_val_loss=True, plot_train_loss=False, draw_min_train_loss=False, draw_min_val_loss=False):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    cluster_path = os.path.join(local_directory, cluster_name)\n",
    "\n",
    "    # Check if the experiment path exists and is a directory\n",
    "\n",
    "    \n",
    "    if len(to_plot) == 0:\n",
    "        # plot all experiments in the cluster\n",
    "        directory_list = [item for item in os.listdir(cluster_path)]\n",
    "    else:\n",
    "        directory_list = [os.path.join(cluster_path, item) for item in to_plot]\n",
    "\n",
    "    for item in directory_list:\n",
    "        item_path = os.path.join(cluster_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            # print('\\t', f'Adding {item_path} to plot')\n",
    "\n",
    "            logfile = os.path.join(item_path, \"main.log\")\n",
    "\n",
    "            param_file = os.path.join(item_path, \"run_parameters.txt\")\n",
    "            learn_rate = extract_learning_rate(param_file)\n",
    "            num_params = extract_parameters(param_file) / 1e6 # convert to millions\n",
    "\n",
    "            name = f'LR: {learn_rate}'\n",
    "\n",
    "            streams, lr_data, train_time = parse_logfile(logfile)\n",
    "            steps, lr_val = lr_data\n",
    "\n",
    "            # Panel 1: losses: both train and val\n",
    "            plt.subplot(131)\n",
    "            xs1, ys1 = streams[\"trl\"] # training loss\n",
    "            ys1 = np.array(ys1)\n",
    "\n",
    "            # smooth out ys using a rolling window\n",
    "            # ys = smooth_moving_average(ys, 21) # optional\n",
    "\n",
    "            xs2, ys2 = streams[\"tel\"] # validation loss\n",
    "\n",
    "            if draw_min_train_loss:\n",
    "                plt.axhline(min(ys1), color='b', linestyle='--')\n",
    "            if draw_min_val_loss:\n",
    "                plt.axhline(min(ys2), color='r', linestyle='--')\n",
    "\n",
    "            if plot_train_loss:\n",
    "                plt.plot(xs1, ys1, label=f'({name}) train loss')\n",
    "\n",
    "            if plot_val_loss:\n",
    "                plt.plot(xs2, ys2, label=f'({name}) val loss')\n",
    "            \n",
    "            if num_params is not None and learn_rate is not None:\n",
    "                print(f\"Params: {num_params:.2f}m | LR : {learn_rate:.4f} | Min Train Loss: {min(ys1):.4f} | Min Val Loss : {min(ys2):.4f} | Train Time : {train_time:.2f} hrs\")\n",
    "            else:\n",
    "                print(f\"Min Train Loss: {min(ys)}\")\n",
    "            \n",
    "            plt.xlabel(\"steps\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.title(\"Loss\")\n",
    "\n",
    "            plt.subplot(132)\n",
    "            if \"eval\" in streams:\n",
    "                xs, ys = streams[\"eval\"] # HellaSwag eval\n",
    "                ys = np.array(ys)\n",
    "                plt.plot(xs, ys, label=f\"({name})\")\n",
    "\n",
    "                plt.xlabel(\"steps\")\n",
    "                plt.ylabel(\"accuracy\")\n",
    "                plt.legend()\n",
    "                plt.title(\"HellaSwag eval\")\n",
    "                # print(\"Max Hellaswag eval:\", max(ys))\n",
    "\n",
    "            plt.subplot(133)\n",
    "            plt.plot(steps, lr_val)\n",
    "            plt.title(\"Learning Rate Schedule\")\n",
    "            plt.xlabel(\"steps\")\n",
    "            plt.ylabel(\"learning rate\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **plot_experiments** takes the following inputs:\n",
    "  - name of a cluster to plot (You must have downlaoded the cluster logs in order for the plotting to work)\n",
    "  - a list containing the experiments you want to plot (leave empty if you want to plot them all)\n",
    "  - two booleans for if you want to plot validation loss or training loss \n",
    "  - a boolean for if you want to label the plots by the learning rate they used\n",
    "    - if set to False, then it will default to using the experiment name (folder name)\n",
    "\n",
    "It will print the minimum training loss and minimum validation loss achieved for each experiment, along with the total number of parameters and learning rate used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiment(cluster_name, experiments_to_plot, \n",
    "                plot_val_loss=False, \n",
    "                plot_train_loss=True, \n",
    "                draw_min_train_loss=True,\n",
    "                draw_min_val_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Do any of your analysis here, or keep tables of models you've already tested!\n",
    "\n",
    "| Example | of | a | table |\n",
    "| --- | --- | --- | --- |\n",
    "| this | is | a | row |\n",
    "| and | this | is | also |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
